---
title: "Use Case: Personal Search"
description: "Building a private, semantic search engine for your own data."
---

Leverage the AI-SDK and Ollama to build a search engine that runs entirely on your local machine. This ensures your knowledge remains private and accessible even offline.

## Ingesting Data

You can use the Worlds CLI to ingest local directories into a World:

```bash
# Ingest a directory of markdown files
worlds ingest ./notes --world my-notes
```

## Private Embeddings

By default, the platform uses **Ollama** running locally. This means your text snippets are never sent to a third-party API for vectorization.

- **Model**: `nomic-embed-text` (default)
- **Engine**: Local Ollama Server

## Querying your Knowledge

Once ingested, you can perform semantic searches that understand context, rather than just keywords:

```typescript
import { createWorldClient } from "@wazootech/worlds-sdk";

const world = createWorldClient("my-notes");
const results = await world.search("How do I set up a local database?");

console.log(results);
```

> [!TIP]
> Learn more about how your data is protected in [Data Privacy](/infrastructure/data-privacy).
